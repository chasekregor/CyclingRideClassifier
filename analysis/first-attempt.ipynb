{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Cycling Ride Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "* [First Bullet Header](#first-bullet)\n",
    "* [Second Bullet Header](#second-bullet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import arff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides = pd.read_csv('/Users/chasekregor/GitHub/2020/CyclingRideClassifier/data/rides.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        activity_id    distance  elevation gain  average_speed  average_watts  \\\ncount  7.240000e+02  724.000000      724.000000     724.000000     676.000000   \nmean   1.469044e+09   15.397818      745.758925     222.438853     123.401331   \nstd    1.121397e+09   12.320441      998.293526    3974.501586      36.227850   \nmin    1.042879e+08    0.000000        0.000000       0.000000       0.000000   \n25%    4.308549e+08    7.695000        0.000000      10.704859     106.075000   \n50%    1.024044e+09   13.000000      406.824147      14.070329     127.600000   \n75%    2.591429e+09   20.000000     1110.564304      17.050488     144.725000   \nmax    3.736531e+09  101.890000     9558.070866   79200.071582     236.200000   \n\n       suffer_score  average_heartrate  average_cadence   kilojoules  \\\ncount    616.000000         618.000000       535.000000   670.000000   \nmean      67.900974         147.064563        78.487290   515.428507   \nstd       57.046422          16.528981        10.017512   410.220105   \nmin        0.000000           0.000000         0.000000     0.000000   \n25%       30.000000         140.650000        73.900000   268.525000   \n50%       53.000000         149.000000        79.800000   429.150000   \n75%       87.000000         155.800000        85.500000   649.525000   \nmax      383.000000         185.600000       101.000000  3193.000000   \n\n       average_temp  start_longitude  start_latitude  \ncount    487.000000       512.000000      512.000000  \nmean      21.080082       -99.553535       37.972109  \nstd        7.892524        18.218156        5.708634  \nmin        0.000000      -160.000000      -36.780000  \n25%       16.000000      -105.280000       36.100000  \n50%       22.000000      -105.010000       39.750000  \n75%       27.000000       -86.850000       40.010000  \nmax       36.000000       175.000000       48.680000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>activity_id</th>\n      <th>distance</th>\n      <th>elevation gain</th>\n      <th>average_speed</th>\n      <th>average_watts</th>\n      <th>suffer_score</th>\n      <th>average_heartrate</th>\n      <th>average_cadence</th>\n      <th>kilojoules</th>\n      <th>average_temp</th>\n      <th>start_longitude</th>\n      <th>start_latitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>count</td>\n      <td>7.240000e+02</td>\n      <td>724.000000</td>\n      <td>724.000000</td>\n      <td>724.000000</td>\n      <td>676.000000</td>\n      <td>616.000000</td>\n      <td>618.000000</td>\n      <td>535.000000</td>\n      <td>670.000000</td>\n      <td>487.000000</td>\n      <td>512.000000</td>\n      <td>512.000000</td>\n    </tr>\n    <tr>\n      <td>mean</td>\n      <td>1.469044e+09</td>\n      <td>15.397818</td>\n      <td>745.758925</td>\n      <td>222.438853</td>\n      <td>123.401331</td>\n      <td>67.900974</td>\n      <td>147.064563</td>\n      <td>78.487290</td>\n      <td>515.428507</td>\n      <td>21.080082</td>\n      <td>-99.553535</td>\n      <td>37.972109</td>\n    </tr>\n    <tr>\n      <td>std</td>\n      <td>1.121397e+09</td>\n      <td>12.320441</td>\n      <td>998.293526</td>\n      <td>3974.501586</td>\n      <td>36.227850</td>\n      <td>57.046422</td>\n      <td>16.528981</td>\n      <td>10.017512</td>\n      <td>410.220105</td>\n      <td>7.892524</td>\n      <td>18.218156</td>\n      <td>5.708634</td>\n    </tr>\n    <tr>\n      <td>min</td>\n      <td>1.042879e+08</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-160.000000</td>\n      <td>-36.780000</td>\n    </tr>\n    <tr>\n      <td>25%</td>\n      <td>4.308549e+08</td>\n      <td>7.695000</td>\n      <td>0.000000</td>\n      <td>10.704859</td>\n      <td>106.075000</td>\n      <td>30.000000</td>\n      <td>140.650000</td>\n      <td>73.900000</td>\n      <td>268.525000</td>\n      <td>16.000000</td>\n      <td>-105.280000</td>\n      <td>36.100000</td>\n    </tr>\n    <tr>\n      <td>50%</td>\n      <td>1.024044e+09</td>\n      <td>13.000000</td>\n      <td>406.824147</td>\n      <td>14.070329</td>\n      <td>127.600000</td>\n      <td>53.000000</td>\n      <td>149.000000</td>\n      <td>79.800000</td>\n      <td>429.150000</td>\n      <td>22.000000</td>\n      <td>-105.010000</td>\n      <td>39.750000</td>\n    </tr>\n    <tr>\n      <td>75%</td>\n      <td>2.591429e+09</td>\n      <td>20.000000</td>\n      <td>1110.564304</td>\n      <td>17.050488</td>\n      <td>144.725000</td>\n      <td>87.000000</td>\n      <td>155.800000</td>\n      <td>85.500000</td>\n      <td>649.525000</td>\n      <td>27.000000</td>\n      <td>-86.850000</td>\n      <td>40.010000</td>\n    </tr>\n    <tr>\n      <td>max</td>\n      <td>3.736531e+09</td>\n      <td>101.890000</td>\n      <td>9558.070866</td>\n      <td>79200.071582</td>\n      <td>236.200000</td>\n      <td>383.000000</td>\n      <td>185.600000</td>\n      <td>101.000000</td>\n      <td>3193.000000</td>\n      <td>36.000000</td>\n      <td>175.000000</td>\n      <td>48.680000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "rides.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "           date                moving_time  activity_id  \\\n386  2017-03-30  0 days 00:00:01.000000000    921397139   \n404  2016-12-31  0 days 00:00:01.000000000    816966884   \n\n                                                  name  distance  \\\n386                       Spin with Abby and Mary Lou       20.0   \n404  Distance: Spin class with Abby's fam. Awesome ...      22.0   \n\n     elevation gain  type  trainer  average_speed  average_watts  \\\n386             0.0  Ride    False   72000.044739            NaN   \n404             0.0  Ride    False   79200.071582            NaN   \n\n     suffer_score  average_heartrate  average_cadence  kilojoules   gear_id  \\\n386           NaN                NaN              NaN         NaN  b1315248   \n404           NaN                NaN              NaN         NaN  b1315248   \n\n     average_temp  start_longitude  start_latitude  device_watts  \n386           NaN              NaN             NaN         False  \n404           NaN              NaN             NaN         False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>moving_time</th>\n      <th>activity_id</th>\n      <th>name</th>\n      <th>distance</th>\n      <th>elevation gain</th>\n      <th>type</th>\n      <th>trainer</th>\n      <th>average_speed</th>\n      <th>average_watts</th>\n      <th>suffer_score</th>\n      <th>average_heartrate</th>\n      <th>average_cadence</th>\n      <th>kilojoules</th>\n      <th>gear_id</th>\n      <th>average_temp</th>\n      <th>start_longitude</th>\n      <th>start_latitude</th>\n      <th>device_watts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>386</td>\n      <td>2017-03-30</td>\n      <td>0 days 00:00:01.000000000</td>\n      <td>921397139</td>\n      <td>Spin with Abby and Mary Lou</td>\n      <td>20.0</td>\n      <td>0.0</td>\n      <td>Ride</td>\n      <td>False</td>\n      <td>72000.044739</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>b1315248</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <td>404</td>\n      <td>2016-12-31</td>\n      <td>0 days 00:00:01.000000000</td>\n      <td>816966884</td>\n      <td>Distance: Spin class with Abby's fam. Awesome ...</td>\n      <td>22.0</td>\n      <td>0.0</td>\n      <td>Ride</td>\n      <td>False</td>\n      <td>79200.071582</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>b1315248</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 125
    }
   ],
   "source": [
    "rides[rides['average_speed']> 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Here the idea is to handle any missing or problematic data. In addition we are going to label and split our data into train, test, and validation splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# not sure why I have to run the cell below twice?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         date                moving_time activity_id  \\\n0  2020-07-09  0 days 01:00:42.000000000  3736531260   \n1  2020-07-06  0 days 00:20:01.000000000  3724400147   \n2  2020-06-21  0 days 00:55:34.000000000  3650328303   \n3  2020-06-16  0 days 00:20:31.000000000  3626733948   \n4  2020-06-13  0 days 01:30:00.000000000  3609777201   \n\n                                                name distance  \\\n0           group workout: ebbetts +2 w dad & hunter    18.24   \n1                                          ramp test     5.66   \n2                                       morning ride    13.17   \n3                                             pettit     5.97   \n4  junction -1: every now and then i think to mys...    25.48   \n\n       elevation gain  type  trainer  average_speed average_watts  \\\n0                 0.0  ride        1      18.025233         154.6   \n1                 0.0  ride        1      16.953740         175.2   \n2  219.81627296587925  ride        0      14.217967          92.3   \n3                 0.0  ride        1      17.452577         153.7   \n4                 0.0  ride        1      16.987294         161.2   \n\n  suffer_score  average_heartrate average_cadence  kilojoules   gear_id  \\\n0         72.0              151.2            87.9       563.0  b4933861   \n1         25.0              151.5            83.1       210.4  b4933861   \n2         19.0              126.8            76.9       307.8  b4933861   \n3         12.0              137.2            85.7       189.3  b4933861   \n4        157.0              159.5            88.8       870.7  b4933861   \n\n         average_temp     start_longitude      start_latitude  device_watts  \n0  21.080082135523615  -99.55353515624923  37.972109375000045             1  \n1  21.080082135523615  -99.55353515624923  37.972109375000045             1  \n2                26.0             -104.97               39.74             1  \n3  21.080082135523615  -99.55353515624923  37.972109375000045             1  \n4  21.080082135523615  -99.55353515624923  37.972109375000045             1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>moving_time</th>\n      <th>activity_id</th>\n      <th>name</th>\n      <th>distance</th>\n      <th>elevation gain</th>\n      <th>type</th>\n      <th>trainer</th>\n      <th>average_speed</th>\n      <th>average_watts</th>\n      <th>suffer_score</th>\n      <th>average_heartrate</th>\n      <th>average_cadence</th>\n      <th>kilojoules</th>\n      <th>gear_id</th>\n      <th>average_temp</th>\n      <th>start_longitude</th>\n      <th>start_latitude</th>\n      <th>device_watts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2020-07-09</td>\n      <td>0 days 01:00:42.000000000</td>\n      <td>3736531260</td>\n      <td>group workout: ebbetts +2 w dad &amp; hunter</td>\n      <td>18.24</td>\n      <td>0.0</td>\n      <td>ride</td>\n      <td>1</td>\n      <td>18.025233</td>\n      <td>154.6</td>\n      <td>72.0</td>\n      <td>151.2</td>\n      <td>87.9</td>\n      <td>563.0</td>\n      <td>b4933861</td>\n      <td>21.080082135523615</td>\n      <td>-99.55353515624923</td>\n      <td>37.972109375000045</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2020-07-06</td>\n      <td>0 days 00:20:01.000000000</td>\n      <td>3724400147</td>\n      <td>ramp test</td>\n      <td>5.66</td>\n      <td>0.0</td>\n      <td>ride</td>\n      <td>1</td>\n      <td>16.953740</td>\n      <td>175.2</td>\n      <td>25.0</td>\n      <td>151.5</td>\n      <td>83.1</td>\n      <td>210.4</td>\n      <td>b4933861</td>\n      <td>21.080082135523615</td>\n      <td>-99.55353515624923</td>\n      <td>37.972109375000045</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2020-06-21</td>\n      <td>0 days 00:55:34.000000000</td>\n      <td>3650328303</td>\n      <td>morning ride</td>\n      <td>13.17</td>\n      <td>219.81627296587925</td>\n      <td>ride</td>\n      <td>0</td>\n      <td>14.217967</td>\n      <td>92.3</td>\n      <td>19.0</td>\n      <td>126.8</td>\n      <td>76.9</td>\n      <td>307.8</td>\n      <td>b4933861</td>\n      <td>26.0</td>\n      <td>-104.97</td>\n      <td>39.74</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2020-06-16</td>\n      <td>0 days 00:20:31.000000000</td>\n      <td>3626733948</td>\n      <td>pettit</td>\n      <td>5.97</td>\n      <td>0.0</td>\n      <td>ride</td>\n      <td>1</td>\n      <td>17.452577</td>\n      <td>153.7</td>\n      <td>12.0</td>\n      <td>137.2</td>\n      <td>85.7</td>\n      <td>189.3</td>\n      <td>b4933861</td>\n      <td>21.080082135523615</td>\n      <td>-99.55353515624923</td>\n      <td>37.972109375000045</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2020-06-13</td>\n      <td>0 days 01:30:00.000000000</td>\n      <td>3609777201</td>\n      <td>junction -1: every now and then i think to mys...</td>\n      <td>25.48</td>\n      <td>0.0</td>\n      <td>ride</td>\n      <td>1</td>\n      <td>16.987294</td>\n      <td>161.2</td>\n      <td>157.0</td>\n      <td>159.5</td>\n      <td>88.8</td>\n      <td>870.7</td>\n      <td>b4933861</td>\n      <td>21.080082135523615</td>\n      <td>-99.55353515624923</td>\n      <td>37.972109375000045</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "#getting rid of rides above 40, those are obviously data errors\n",
    "rides = rides[rides['average_speed']< 40]\n",
    "\n",
    "\n",
    "#rides['moving_time'] = rides['moving_time'].dt.total_seconds().div(60).astype(int)\n",
    "\n",
    "#fill all the None's the average of their respective columns\n",
    "rides = rides.fillna(rides.mean())\n",
    "\n",
    "rides = rides.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "\n",
    "#for some reason I can't get the none in average_temp to come out so I simply deleting those rows\n",
    "rides = rides[rides['average_temp'] != 'none']\n",
    "\n",
    "#changing some of the features we want to use to type float\n",
    "rides.average_speed = rides.average_speed.astype(float)\n",
    "rides.average_heartrate = rides.average_heartrate.astype(float)\n",
    "rides.kilojoules = rides.kilojoules.astype(float)\n",
    "\n",
    "rides = rides.replace('true',\"1\")\n",
    "rides = rides.replace('false',\"0\")\n",
    "\n",
    "rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "road       577\npeloton     99\nmtb         46\nName: classification, dtype: int64"
     },
     "metadata": {},
     "execution_count": 127
    }
   ],
   "source": [
    "conditions = [\n",
    "    (rides['gear_id'] == 'b3092328'),\n",
    "    (rides['gear_id'] == 'b5499491'),\n",
    "    (rides['gear_id'] != 'b3092328') & (rides['gear_id'] != 'b5499491')\n",
    "    ]\n",
    "choices = ['mtb','peloton','road']\n",
    "rides['classification'] = np.select(conditions, choices, default=None)\n",
    "rides.classification.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "xVar = list(['distance','average_speed','average_heartrate','kilojoules','elevation gain','suffer_score','trainer','average_cadence','average_watts','average_temp','device_watts'])\n",
    "yVar = rides['classification']\n",
    "df2 = rides[xVar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(577, 11) (577,)\n(145, 11) (145,)\n"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df2, yVar, test_size=0.2)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Predicted Result  mtb  peloton  road\nActual Result                       \nmtb                 6        0     1\npeloton             0       11     4\nroad                0        5   118\n\n              precision    recall  f1-score   support\n\n         mtb       1.00      0.86      0.92         7\n     peloton       0.69      0.73      0.71        15\n        road       0.96      0.96      0.96       123\n\n    accuracy                           0.93       145\n   macro avg       0.88      0.85      0.86       145\nweighted avg       0.93      0.93      0.93       145\n\n0.9310344827586207\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('distance', 0.05752457093751591),\n ('average_speed', 0.2065260767866693),\n ('average_heartrate', 0.06309483119043494),\n ('kilojoules', 0.11320414847281976),\n ('elevation gain', 0.061339314697498794),\n ('suffer_score', 0.06231705958893115),\n ('trainer', 0.14834764738583936),\n ('average_cadence', 0.08088901452684039),\n ('average_watts', 0.09395007486922188),\n ('average_temp', 0.07790792251800607),\n ('device_watts', 0.034899339026222456)]"
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=10, n_jobs=2, oob_score=False, random_state=0,\n",
    "            verbose=0, warm_start=False)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(pd.crosstab(y_test, y_pred, rownames=['Actual Result'], colnames=['Predicted Result']))\n",
    "print(\"\")\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"\")\n",
    "list(zip(X_train, clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors (k-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'n_neighbors': 5}\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#create new a knn model\n",
    "knn = KNeighborsClassifier()\n",
    "#create a dictionary of all values we want to test for n_neighbors\n",
    "params_knn = {'n_neighbors': np.arange(1, 25)}\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
    "#fit model to training data\n",
    "knn_gs.fit(X_train, y_train)\n",
    "#save best model\n",
    "knn_best = knn_gs.best_estimator_\n",
    "#check best n_neigbors value\n",
    "print(knn_gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'n_estimators': 100}\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#create a new random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "#create a dictionary of all values we want to test for n_estimators\n",
    "params_rf = {'n_estimators': [50, 100, 200]}\n",
    "#use gridsearch to test all values for n_estimators\n",
    "rf_gs = GridSearchCV(rf, params_rf, cv=5)\n",
    "#fit model to training data\n",
    "rf_gs.fit(X_train, y_train)\n",
    "#save best model\n",
    "rf_best = rf_gs.best_estimator_\n",
    "#check best n_estimators value\n",
    "print(rf_gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False)"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "#create a new logistic regression model\n",
    "log_reg = LogisticRegression()\n",
    "#fit the model to the training data\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('standardscaler',\n                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n                ('svc',\n                 SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                     decision_function_shape='ovr', degree=3, gamma='auto',\n                     kernel='rbf', max_iter=-1, probability=False,\n                     random_state=None, shrinking=True, tol=0.001,\n                     verbose=False))],\n         verbose=False)"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "SVC = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "\n",
    "SVC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the models scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "knn: 0.8068965517241379\nrf: 0.896551724137931\nlog_reg: 0.8\nSVC: 0.8620689655172413\n"
    }
   ],
   "source": [
    "#test the three models with the test data and print their accuracy scores\n",
    "print('knn: {}'.format(knn_best.score(X_test, y_test)))\n",
    "print('rf: {}'.format(rf_best.score(X_test, y_test)))\n",
    "print('log_reg: {}'.format(log_reg.score(X_test, y_test)))\n",
    "print('SVC: {}'.format(SVC.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "#create a dictionary of our models\n",
    "estimators=[('knn', knn_best), ('random forrests', rf_best), ('log_reg', log_reg), ('SVC', SVC)]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8620689655172413"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "#fit model to training data\n",
    "ensemble.fit(X_train, y_train)\n",
    "#test our model on the test data\n",
    "ensemble.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[  0   0  10]\n [  0   0  19]\n [  0   0 116]]\n              precision    recall  f1-score   support\n\n         mtb       0.00      0.00      0.00        10\n     peloton       0.00      0.00      0.00        19\n        road       0.80      1.00      0.89       116\n\n    accuracy                           0.80       145\n   macro avg       0.27      0.33      0.30       145\nweighted avg       0.64      0.80      0.71       145\n\n0.8\n"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 10000, 3), max_iter=1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network with Keras and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     distance  average_speed  average_heartrate   kilojoules  elevation gain  \\\n0       18.24      18.025233         151.200000   563.000000        0.000000   \n1        5.66      16.953740         151.500000   210.400000        0.000000   \n2       13.17      14.217967         126.800000   307.800000      219.816273   \n3        5.97      17.452577         137.200000   189.300000        0.000000   \n4       25.48      16.987294         159.500000   870.700000        0.000000   \n..        ...            ...                ...          ...             ...   \n719     11.00       9.498031         147.064563   247.200000      298.228346   \n720     45.55      12.495526         147.064563  1341.600000     2792.979003   \n721     28.20      11.983268         147.064563   998.000000     2025.918635   \n722     20.24       9.634485         147.064563  1056.700000     2527.887139   \n723     17.19      13.891374         147.064563   515.428507      775.590551   \n\n     suffer_score  trainer  average_cadence  average_watts  average_temp  \\\n0       72.000000        1         87.90000     154.600000     21.080082   \n1       25.000000        1         83.10000     175.200000     21.080082   \n2       19.000000        0         76.90000      92.300000     26.000000   \n3       12.000000        1         85.70000     153.700000     21.080082   \n4      157.000000        1         88.80000     161.200000     21.080082   \n..            ...      ...              ...            ...           ...   \n719     67.900974        0         78.48729      59.300000     21.080082   \n720     67.900974        0         78.48729     102.200000     21.080082   \n721     67.900974        0         78.48729     117.800000     21.080082   \n722     67.900974        0         78.48729     139.700000     21.080082   \n723     67.900974        0         78.48729     123.401331     21.080082   \n\n     device_watts  classification  \n0               1             NaN  \n1               1             NaN  \n2               1             NaN  \n3               1             NaN  \n4               1             NaN  \n..            ...             ...  \n719             0             NaN  \n720             0             NaN  \n721             0             NaN  \n722             0             NaN  \n723             0             NaN  \n\n[722 rows x 12 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>distance</th>\n      <th>average_speed</th>\n      <th>average_heartrate</th>\n      <th>kilojoules</th>\n      <th>elevation gain</th>\n      <th>suffer_score</th>\n      <th>trainer</th>\n      <th>average_cadence</th>\n      <th>average_watts</th>\n      <th>average_temp</th>\n      <th>device_watts</th>\n      <th>classification</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>18.24</td>\n      <td>18.025233</td>\n      <td>151.200000</td>\n      <td>563.000000</td>\n      <td>0.000000</td>\n      <td>72.000000</td>\n      <td>1</td>\n      <td>87.90000</td>\n      <td>154.600000</td>\n      <td>21.080082</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>5.66</td>\n      <td>16.953740</td>\n      <td>151.500000</td>\n      <td>210.400000</td>\n      <td>0.000000</td>\n      <td>25.000000</td>\n      <td>1</td>\n      <td>83.10000</td>\n      <td>175.200000</td>\n      <td>21.080082</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>13.17</td>\n      <td>14.217967</td>\n      <td>126.800000</td>\n      <td>307.800000</td>\n      <td>219.816273</td>\n      <td>19.000000</td>\n      <td>0</td>\n      <td>76.90000</td>\n      <td>92.300000</td>\n      <td>26.000000</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>5.97</td>\n      <td>17.452577</td>\n      <td>137.200000</td>\n      <td>189.300000</td>\n      <td>0.000000</td>\n      <td>12.000000</td>\n      <td>1</td>\n      <td>85.70000</td>\n      <td>153.700000</td>\n      <td>21.080082</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>25.48</td>\n      <td>16.987294</td>\n      <td>159.500000</td>\n      <td>870.700000</td>\n      <td>0.000000</td>\n      <td>157.000000</td>\n      <td>1</td>\n      <td>88.80000</td>\n      <td>161.200000</td>\n      <td>21.080082</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>719</td>\n      <td>11.00</td>\n      <td>9.498031</td>\n      <td>147.064563</td>\n      <td>247.200000</td>\n      <td>298.228346</td>\n      <td>67.900974</td>\n      <td>0</td>\n      <td>78.48729</td>\n      <td>59.300000</td>\n      <td>21.080082</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>45.55</td>\n      <td>12.495526</td>\n      <td>147.064563</td>\n      <td>1341.600000</td>\n      <td>2792.979003</td>\n      <td>67.900974</td>\n      <td>0</td>\n      <td>78.48729</td>\n      <td>102.200000</td>\n      <td>21.080082</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>721</td>\n      <td>28.20</td>\n      <td>11.983268</td>\n      <td>147.064563</td>\n      <td>998.000000</td>\n      <td>2025.918635</td>\n      <td>67.900974</td>\n      <td>0</td>\n      <td>78.48729</td>\n      <td>117.800000</td>\n      <td>21.080082</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>722</td>\n      <td>20.24</td>\n      <td>9.634485</td>\n      <td>147.064563</td>\n      <td>1056.700000</td>\n      <td>2527.887139</td>\n      <td>67.900974</td>\n      <td>0</td>\n      <td>78.48729</td>\n      <td>139.700000</td>\n      <td>21.080082</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>723</td>\n      <td>17.19</td>\n      <td>13.891374</td>\n      <td>147.064563</td>\n      <td>515.428507</td>\n      <td>775.590551</td>\n      <td>67.900974</td>\n      <td>0</td>\n      <td>78.48729</td>\n      <td>123.401331</td>\n      <td>21.080082</td>\n      <td>0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>722 rows Ã— 12 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "xVar = list(['distance','average_speed','average_heartrate','kilojoules','elevation gain','suffer_score','trainer','average_cadence','average_watts','average_temp','device_watts','classification'])\n",
    "rides = rides[xVar]\n",
    "rides = rides.apply(pd.to_numeric, errors='coerce')\n",
    "df[\"A\"] = pd.to_numeric(df[\"A\"], downcast=\"float\")\n",
    "rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "distance             float64\naverage_speed        float64\naverage_heartrate    float64\nkilojoules           float64\nelevation gain       float64\nsuffer_score         float64\ntrainer                int64\naverage_cadence      float64\naverage_watts        float64\naverage_temp         float64\ndevice_watts           int64\nclassification       float64\ndtype: object"
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "rides.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = rides.pop('classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-4b739a17840a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \"\"\"\n\u001b[0;32m--> 640\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   2856\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2858\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2859\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2860\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 115\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m    116\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m   \"\"\"\n\u001b[1;32m    261\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 262\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((rides.values, classification.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nfc1 (Dense)                  (None, 16)                80        \n_________________________________________________________________\nfc2 (Dense)                  (None, 3)                 51        \n=================================================================\nTotal params: 131\nTrainable params: 131\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='sigmoid', \n",
    "                          name='fc1', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, name='fc2', activation='softmax')])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'shuffle'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-e5119ca7f387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'shuffle'"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "training_size = 100\n",
    "batch_size = 2\n",
    "steps_per_epoch = np.ceil(training_size / batch_size)\n",
    "\n",
    "ds_train = X_train.shuffle(buffer_size=training_size)\n",
    "ds_train = ds_train.repeat()\n",
    "ds_train = ds_train.batch(batch_size=batch_size)\n",
    "ds_train = ds_train.prefetch(buffer_size=1000)\n",
    "\n",
    "history = model.fit(ds_train, epochs=num_epochs,\n",
    "                         steps_per_epoch=steps_per_epoch, \n",
    "                         verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history.history\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.plot(hist['loss'], lw=3)\n",
    "ax.set_title('Training loss', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.plot(hist['accuracy'], lw=3)\n",
    "ax.set_title('Training accuracy', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('ch13-cls-learning-curve.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}